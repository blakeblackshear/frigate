# syntax=docker/dockerfile:1.4

# https://askubuntu.com/questions/972516/debian-frontend-environment-variable
ARG DEBIAN_FRONTEND=noninteractive

ARG BASE_IMAGE
FROM ${BASE_IMAGE} AS build-wheels
ARG DEBIAN_FRONTEND

# Use a separate container to build wheels to prevent build dependencies in final image
RUN apt-get -qq update \
    && apt-get -qq install -y --no-install-recommends \
    wget build-essential cmake git \
    && rm -rf /var/lib/apt/lists/*


FROM build-wheels AS trt-wheels
ARG DEBIAN_FRONTEND
ARG TARGETARCH

# Determine version of tensorrt already installed in base image, e.g. "Version: 8.4.1-1+cuda11.4"
RUN NVINFER_VER=$(dpkg -s libnvinfer10 | grep -Po "Version: \K.*") \
    && echo $NVINFER_VER | grep -Po "^\d+\.\d+\.\d+" > /etc/TENSORRT_VER

RUN --mount=type=bind,source=docker/tensorrt/detector/build_python_tensorrt.sh,target=/deps/build_python_tensorrt.sh \
    --mount=type=cache,target=/root/.ccache \
    export PATH="/usr/lib/ccache:$PATH" CCACHE_DIR=/root/.ccache CCACHE_MAXSIZE=2G \
    && TENSORRT_VER=$(cat /etc/TENSORRT_VER) /deps/build_python_tensorrt.sh

COPY docker/tensorrt/requirements-arm64.txt /requirements-tensorrt.txt
# See https://elinux.org/Jetson_Zoo#ONNX_Runtime
ADD https://nvidia.box.com/shared/static/6l0u97rj80ifwkk8rqbzj1try89fk26z.whl /tmp/onnxruntime_gpu-1.19.0-cp310-cp310-linux_aarch64.whl

RUN pip3 uninstall -y onnxruntime-openvino \
    && pip3 wheel --wheel-dir=/trt-wheels -r /requirements-tensorrt.txt \
    && pip3 install --no-deps /tmp/onnxruntime_gpu-1.19.0-cp310-cp310-linux_aarch64.whl

FROM build-wheels AS trt-model-wheels
ARG DEBIAN_FRONTEND

RUN apt-get update \
    && apt-get install -y protobuf-compiler libprotobuf-dev \
    && rm -rf /var/lib/apt/lists/*
RUN --mount=type=bind,source=docker/tensorrt/requirements-models-arm64.txt,target=/requirements-tensorrt-models.txt \
    pip3 wheel --wheel-dir=/trt-model-wheels -r /requirements-tensorrt-models.txt

FROM wget AS jetson-ffmpeg
ARG DEBIAN_FRONTEND
ENV CCACHE_DIR /root/.ccache
ENV CCACHE_MAXSIZE 2G
RUN --mount=type=bind,source=docker/tensorrt/build_jetson_ffmpeg.sh,target=/deps/build_jetson_ffmpeg.sh \
    --mount=type=cache,target=/root/.ccache \
    /deps/build_jetson_ffmpeg.sh

# Frigate w/ TensorRT for NVIDIA Jetson platforms
FROM tensorrt-base AS frigate-tensorrt
RUN apt-get update \
    && apt-get install -y python-is-python3 libprotobuf23  \
    && rm -rf /var/lib/apt/lists/*

RUN rm -rf /usr/lib/btbn-ffmpeg/
COPY --from=jetson-ffmpeg /rootfs /

COPY --from=trt-wheels /etc/TENSORRT_VER /etc/TENSORRT_VER
RUN --mount=type=bind,from=trt-wheels,source=/trt-wheels,target=/deps/trt-wheels \
    --mount=type=bind,from=trt-model-wheels,source=/trt-model-wheels,target=/deps/trt-model-wheels \
    pip3 install -U /deps/trt-wheels/*.whl /deps/trt-model-wheels/*.whl \
    && ldconfig

WORKDIR /opt/frigate/
COPY --from=rootfs / /
